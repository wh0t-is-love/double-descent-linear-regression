# Двойной спуск на линейной регрессии
## О задаче
В нейронных сетях есть эффект, который называется "Двойной спуск". Оказывается, его можно также наблюдать и на линейной регрессии. и на этой модели можно сделать много выводов, которые могут быть применимы также и в нейронных сетях.
Ниже кратко о выводах, которые можно сделать из данного исследования:
## Файл LinearDD_base:
- Пик тестовой ошибки наблюдается при количестве параметров равному количеству обучающей выборки.
- Норма весов повторяет поведение ошибки на тесте.
- При применении стохастического градиентного спуска результаты не меняются, хотя их становится намного сложнее получить.
- При добавлении L2 регуляризации и увеличении коэффициента регуляризации пик ошибки уменьшается, пока совсем не пропадет.

## Файл LinearDD2_all:
Норма весов примерно одинаковая для всех подходов, но с небольшим «но». Результат очень сильно зависит от изначально подобранных параметров для итеративных методов (количество эпох, величина шага). Норма весов продолжает повторять поведение ошибки на тесте для каждого из подходов. В критическом режиме (вблизи максимума) норма весов у итеративных подходов и аналитического может отличаться, но в недопараметризованной и перепараметризованных моделях норма ведет себя практически одинаково. Почему это так происходит подробно расписано в следующем пункте
## Файлы LinearDD2_variance, LinearDD2_iters:
Нужно исследовать поведение функции потерь и объяснить происходящие эффекты.

Для начала нужно сказать, что MSE – выпуклая функция, а это значит, что она имеет один минимум и он будет глобальным. Т.е. отрицание градиента всегда будет направлено к этому единственному оптимуму. Если мы возьмем размер шага слишком большим, то регрессия просто разойдется, потому что мы «выпрыгнем» из нужного минимума. Если теперь мы возьмем подходящий learning rate (регрессия не расходится) и для разных значений learning rate зафиксируем количество эпох, то увидим любопытный эффект: при уменьшении величины шага (learning rate) будет уменьшаться пик ошибки в критическом режиме, однако в недопараметризованной и перепараметризованных моделях ошибка примерно одинаковая. Почему это происходит? Первопричина – мы недообучаемся, то есть мы не успеваем дойти до нужного минимума, а количество эпох уже закончилось. Но почему это так сильно заметно именно в критическом режиме? Это связано с тем, что дисперсия в этот момент также достигает своего максимума, как и тестовая функция потерь, и норма весов, а смещение (bias) достигает своего минимума, как и ошибка на обучающей выборке. И теперь достаточно небольшая разница в аналитическом решении и итеративных подходах очень сильно влияет на функцию потерь. Итак, если мы берем слишком мало эпох для маленького learning rate, то модель не успевает находить нужный минимум и это особенно видно в критическом режиме.

Также можно заметить, что величина шага у градиентного спуска больше чем у стохастического градиента. Это связано как раз с тем, что по сути градиентный спуск – это стохастический градиентный спуск с батчем равном всей выборке. Причем при градиентном спуске мы рано или поздно дойдем до оптимума, потому что общий градиент будет уменьшаться с каждым шагом. А при увеличении размера батча падает дисперсия. Также в SGD (в моем случае батч равен 1) веса претерпевают в N раз больше изменений.

Конечно, в идеале нужно иметь бесконечно большое число итераций и бесконечно малое значение learning rate чтобы достичь наш оптимум и расхождений с аналитическим решением не будет.
Файл с этими исследованиями - LinearDD2_all. Также в доказательство «несходимости» из-за недообученности модели (когда число эпох мало) (файл LinearDD2_iters). В этих файлах бралось разное количество эпох и разный learning rate. Конкретно тут взят пример с lr=1e-6 (1e4 и 1e5 эпох) и lr=1e7 (1e5 эпох), а графики с lr=1e-6 (1e4 эпох) и lr=1e7 (1e5 эпох) ведут себя практически одинаково, хотя lr=1e-6 (1e5 эпох) достигает лучших минимумов. Это значит, что при маленьких значениях lr нужно большее число эпох, так как он не успевает сойтись к нужному оптимуму.

